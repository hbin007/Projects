import streamlit as st
from PIL import Image
import matplotlib
from selenium import webdriver
from bs4 import BeautifulSoup
import requests
import numpy as np
import pandas as pd
import time
import multiprocessing
from functools import partial
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException
from fake_useragent import UserAgent
import random


# ë¬´ì‹ ì‚¬ íŒ¨ìŠ¤ì¤€ë¹„
def musinsa_url(key_word, page_num):
    print(f"'{key_word}'ê²€ìƒ‰ ì‹œì‘ {page_num}í˜ì´ì§€")
    path = []
    ua = UserAgent()
    headers = {
         'User-Agent': ua.random}

    for num in range(1, page_num + 1):
        url = f"https://www.musinsa.com/search/musinsa/goods?q={key_word}&list_kind=small&sortCode=pop&sub_sort=&" \
              f"page={num}&display_cnt=0&saleGoods=&includeSoldOut=&setupGoods=&popular=&category1DepthCode=&category" \
              f"2DepthCodes=&category3DepthCodes=&selectedFilters=&category1DepthName=&category2DepthName=&brandIds=" \
              f"&price=&colorCodes=&contentType=&styleTypes=&includeKeywords=&excludeKeywords=&originalYn=N&tags=" \
              f"&campaignId=&serviceType=&eventType=&type=&season=&measure=&openFilterLayout=N&selectedOrderMeasure=" \
              f"&shoeSizeOption=&groupSale=&d_cat_cd=&attribute="
        res = requests.get(url, headers=headers)
        print(res)
        print(f"{num}ë²ˆì§¸ í¬ë¡¤ë§")

        product_url = BeautifulSoup(res.text, 'html.parser')
        li_list = product_url.find_all("li", class_="li_box")

        if len(li_list) == 0:
            break

        for i in range(len(li_list)):
            path.append(li_list[i])
    return path


# ìƒí’ˆ url í¬ë¡¤ë§
def url_crawling(i, f_url):
    global li11
    global li00
    item_info = i.find('a', class_="img-block")
    li11.append(item_info['href'])
    li00.append("ìƒí’ˆurl")
    f_url.append(item_info['href'])

    return f_url


# ìƒí’ˆ name í¬ë¡¤ë§
def name_crawling(i):
    global li11
    global li00
    item_info = i.find('a', class_="img-block")
    li11.append(item_info['title'])
    li00.append("ì œí’ˆëª…")


# ìƒí’ˆ ê°€ê²© í¬ë¡¤ë§
def price_crawling(i):
    global li11
    global li00
    item_info = i.find('a', class_="img-block")
    li11.append(item_info['data-bh-content-meta2'])
    li00.append("ê°€ê²©")


# ìƒí’ˆ í• ì¸ ê°€ê²© í¬ë¡¤ë§
def sale_price_crawling(i):
    global li11
    global li00
    item_info = i.find('a', class_="img-block")
    li11.append(item_info['data-bh-content-meta3'])
    li00.append("í• ì¸ê°€ê²©")


# ìƒí’ˆ ë¸Œëœë“œ í¬ë¡¤ë§
def brand_crawling(i):
    global li11
    global li00
    li11.append(i.find('p', class_="item_title").text)
    li00.append("ë¸Œëœë“œ")


# í¬ë¡¤ë§ ì´ë²¤íŠ¸ ì²˜ë¦¬
def crawling(path, url_box, name_box, price_box, sale_price_box, brand_box):
    global li11
    global li00
    f_url = []
    new_dic = []
    for i in path:
        if url_box:
            f_url = url_crawling(i, f_url)
        if name_box:
            name_crawling(i)
        if price_box:
            price_crawling(i)
        if sale_price_box:
            sale_price_crawling(i)
        if brand_box:
            brand_crawling(i)
        new_dic.append(dict(zip(li00, li11)))
    return new_dic, f_url


# ì„¸ë¶€ì •ë³´ í¬ë¡¤ë§
def details_crawling(f_url,  code):
    rand_value = random.randint(1, 3)
    ua = UserAgent()
    headers = {
        "user-agent": ua.random}

    options = webdriver.ChromeOptions()
    # options.add_argument('--headless')
    options.add_argument('window-size=1920x1080')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    # options.add_argument("--proxy-server=socks5://127.0.0.1:9150")
    options.add_argument('disable-gpu')
    options.add_argument(f'user-agent={headers}')
    options.add_experimental_option("excludeSwitches", ["enable-logging"])
    prefs = {'profile.default_content_setting_values': {'cookies': 2, 'images': 2, 'plugins': 2, 'popups': 2,
                                                        'geolocation': 2, 'notifications': 2,
                                                        'auto_select_certificate': 2,
                                                        'fullscreen': 2, 'mouselock': 2, 'mixed_script': 2,
                                                        'media_stream': 2,
                                                        'media_stream_mic': 2, 'media_stream_camera': 2,
                                                        'protocol_handlers': 2,
                                                        'ppapi_broker': 2, 'automatic_downloads': 2, 'midi_sysex': 2,
                                                        'push_messaging': 2,
                                                        'ssl_cert_decisions': 2, 'metro_switch_to_desktop': 2,
                                                        'protected_media_identifier': 2,
                                                        'app_banner': 2, 'site_engagement': 2, 'durable_storage': 2}}
    options.add_experimental_option('prefs', prefs)

    li2 = []

    driver = webdriver.Chrome('chromedriver', options=options)

    half = len(f_url) // 2
    a1_url = f_url[:half]
    a2_url = f_url[half:]

    half2 = len(a1_url) // 2
    b1_url = a1_url[:half2]
    b2_url = a1_url[half2:]

    half3 = len(a2_url) // 2
    b3_url = a2_url[:half3]
    b4_url = a2_url[half3:]
    if code == 1:
        for site1 in b1_url:
            dic = {}
            driver.get(site1)
            time.sleep(rand_value)
            try:
                try:
                    if driver.find_element(By.XPATH,
                                           '//*[@id="page_product_detail"]/div[3]/div[6]'
                                           '/ul/li[1]').text == "Purchase statusêµ¬ë§¤ í˜„í™©":
                        driver.find_element(
                            By.XPATH, '//*[@id="page_product_detail"]/div[3]/div[6]/ul/li[1]').click()
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[0].get_text()
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[1].get_text()
                        dic["~18ì„¸"] = html.find_all("span", class_="bar_num")[
                            0].get_text()
                        dic["19ì„¸~23ì„¸"] = html.find_all(
                            "span", class_="bar_num")[1].get_text()
                        dic["24ì„¸~28ì„¸"] = html.find_all(
                            "span", class_="bar_num")[2].get_text()
                        dic["29ì„¸~33ì„¸"] = html.find_all(
                            "span", class_="bar_num")[3].get_text()
                        dic["34ì„¸~39ì„¸"] = html.find_all(
                            "span", class_="bar_num")[4].get_text()
                        dic["40ì„¸~"] = html.find_all("span", class_="bar_num")[
                            5].get_text()
                        li2.append(dic)

                    else:
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["~18ì„¸"] = 0
                        dic["19ì„¸~23ì„¸"] = 0
                        dic["24ì„¸~28ì„¸"] = 0
                        dic["29ì„¸~33ì„¸"] = 0
                        dic["34ì„¸~39ì„¸"] = 0
                        dic["40ì„¸~"] = 0
                        li2.append(dic)

                except NoSuchElementException:
                    data = driver.page_source
                    html = BeautifulSoup(data, "html.parser")
                    dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                    dic["ëˆ„ì íŒë§¤"] = html.find("strong", id="sales_1y_qty").text
                    dic["ì¢‹ì•„ìš”"] = html.find("span", class_="prd_like_cnt").text
                    dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                    dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["~18ì„¸"] = 0
                    dic["19ì„¸~23ì„¸"] = 0
                    dic["24ì„¸~28ì„¸"] = 0
                    dic["29ì„¸~33ì„¸"] = 0
                    dic["34ì„¸~39ì„¸"] = 0
                    dic["40ì„¸~"] = 0
                    li2.append(dic)
            except AttributeError:
                print("1ë²ˆ error")
                continue
            print("1ë²ˆ í”„ë¡œì„¸ìŠ¤")

    elif code == 2:
        for site2 in b2_url:
            dic = {}
            driver.get(site2)
            time.sleep(rand_value)
            try:
                try:
                    if driver.find_element(By.XPATH,
                                           '//*[@id="page_product_detail"]/div[3]/div[6]'
                                           '/ul/li[1]').text == "Purchase statusêµ¬ë§¤ í˜„í™©":
                        driver.find_element(
                            By.XPATH, '//*[@id="page_product_detail"]/div[3]/div[6]/ul/li[1]').click()
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[0].get_text()
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[1].get_text()
                        dic["~18ì„¸"] = html.find_all("span", class_="bar_num")[
                            0].get_text()
                        dic["19ì„¸~23ì„¸"] = html.find_all(
                            "span", class_="bar_num")[1].get_text()
                        dic["24ì„¸~28ì„¸"] = html.find_all(
                            "span", class_="bar_num")[2].get_text()
                        dic["29ì„¸~33ì„¸"] = html.find_all(
                            "span", class_="bar_num")[3].get_text()
                        dic["34ì„¸~39ì„¸"] = html.find_all(
                            "span", class_="bar_num")[4].get_text()
                        dic["40ì„¸~"] = html.find_all("span", class_="bar_num")[
                            5].get_text()
                        li2.append(dic)

                    else:
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["~18ì„¸"] = 0
                        dic["19ì„¸~23ì„¸"] = 0
                        dic["24ì„¸~28ì„¸"] = 0
                        dic["29ì„¸~33ì„¸"] = 0
                        dic["34ì„¸~39ì„¸"] = 0
                        dic["40ì„¸~"] = 0
                        li2.append(dic)

                except NoSuchElementException:
                    data = driver.page_source
                    html = BeautifulSoup(data, "html.parser")
                    dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                    dic["ëˆ„ì íŒë§¤"] = html.find("strong", id="sales_1y_qty").text
                    dic["ì¢‹ì•„ìš”"] = html.find("span", class_="prd_like_cnt").text
                    dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                    dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["~18ì„¸"] = 0
                    dic["19ì„¸~23ì„¸"] = 0
                    dic["24ì„¸~28ì„¸"] = 0
                    dic["29ì„¸~33ì„¸"] = 0
                    dic["34ì„¸~39ì„¸"] = 0
                    dic["40ì„¸~"] = 0
                    li2.append(dic)
            except AttributeError:
                print("2ë²ˆ error")
                continue
            print("2ë²ˆ í”„ë¡œì„¸ìŠ¤")

    elif code == 3:
        for site3 in b3_url:
            dic = {}
            driver.get(site3)
            time.sleep(rand_value)
            try:
                try:
                    if driver.find_element(By.XPATH,
                                           '//*[@id="page_product_detail"]/div[3]/div[6]'
                                           '/ul/li[1]').text == "Purchase statusêµ¬ë§¤ í˜„í™©":
                        driver.find_element(
                            By.XPATH, '//*[@id="page_product_detail"]/div[3]/div[6]/ul/li[1]').click()
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[0].get_text()
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[1].get_text()
                        dic["~18ì„¸"] = html.find_all("span", class_="bar_num")[
                            0].get_text()
                        dic["19ì„¸~23ì„¸"] = html.find_all(
                            "span", class_="bar_num")[1].get_text()
                        dic["24ì„¸~28ì„¸"] = html.find_all(
                            "span", class_="bar_num")[2].get_text()
                        dic["29ì„¸~33ì„¸"] = html.find_all(
                            "span", class_="bar_num")[3].get_text()
                        dic["34ì„¸~39ì„¸"] = html.find_all(
                            "span", class_="bar_num")[4].get_text()
                        dic["40ì„¸~"] = html.find_all("span", class_="bar_num")[
                            5].get_text()
                        li2.append(dic)

                    else:
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["~18ì„¸"] = 0
                        dic["19ì„¸~23ì„¸"] = 0
                        dic["24ì„¸~28ì„¸"] = 0
                        dic["29ì„¸~33ì„¸"] = 0
                        dic["34ì„¸~39ì„¸"] = 0
                        dic["40ì„¸~"] = 0
                        li2.append(dic)

                except NoSuchElementException:
                    data = driver.page_source
                    html = BeautifulSoup(data, "html.parser")
                    dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                    dic["ëˆ„ì íŒë§¤"] = html.find("strong", id="sales_1y_qty").text
                    dic["ì¢‹ì•„ìš”"] = html.find("span", class_="prd_like_cnt").text
                    dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                    dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["~18ì„¸"] = 0
                    dic["19ì„¸~23ì„¸"] = 0
                    dic["24ì„¸~28ì„¸"] = 0
                    dic["29ì„¸~33ì„¸"] = 0
                    dic["34ì„¸~39ì„¸"] = 0
                    dic["40ì„¸~"] = 0
                    li2.append(dic)
            except AttributeError:
                print("3ë²ˆ error")
                continue
            print("3ë²ˆ í”„ë¡œì„¸ìŠ¤")

    else:
        for site4 in b4_url:
            dic = {}
            driver.get(site4)
            time.sleep(rand_value)
            try:
                try:
                    if driver.find_element(By.XPATH,
                                           '//*[@id="page_product_detail"]/div[3]/div[6]'
                                           '/ul/li[1]').text == "Purchase statusêµ¬ë§¤ í˜„í™©":
                        driver.find_element(
                            By.XPATH, '//*[@id="page_product_detail"]/div[3]/div[6]/ul/li[1]').click()
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[0].get_text()
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = html.find_all(
                            "dd", class_="label_info_value")[1].get_text()
                        dic["~18ì„¸"] = html.find_all("span", class_="bar_num")[
                            0].get_text()
                        dic["19ì„¸~23ì„¸"] = html.find_all(
                            "span", class_="bar_num")[1].get_text()
                        dic["24ì„¸~28ì„¸"] = html.find_all(
                            "span", class_="bar_num")[2].get_text()
                        dic["29ì„¸~33ì„¸"] = html.find_all(
                            "span", class_="bar_num")[3].get_text()
                        dic["34ì„¸~39ì„¸"] = html.find_all(
                            "span", class_="bar_num")[4].get_text()
                        dic["40ì„¸~"] = html.find_all("span", class_="bar_num")[
                            5].get_text()
                        li2.append(dic)

                    else:
                        data = driver.page_source
                        html = BeautifulSoup(data, "html.parser")
                        dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                        dic["ëˆ„ì íŒë§¤"] = html.find(
                            "strong", id="sales_1y_qty").text
                        dic["ì¢‹ì•„ìš”"] = html.find(
                            "span", class_="prd_like_cnt").text
                        dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                        dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                        dic["~18ì„¸"] = 0
                        dic["19ì„¸~23ì„¸"] = 0
                        dic["24ì„¸~28ì„¸"] = 0
                        dic["29ì„¸~33ì„¸"] = 0
                        dic["34ì„¸~39ì„¸"] = 0
                        dic["40ì„¸~"] = 0
                        li2.append(dic)

                except NoSuchElementException:
                    data = driver.page_source
                    html = BeautifulSoup(data, "html.parser")
                    dic["ì¡°íšŒìˆ˜"] = html.find("strong", id="pageview_1m").text
                    dic["ëˆ„ì íŒë§¤"] = html.find("strong", id="sales_1y_qty").text
                    dic["ì¢‹ì•„ìš”"] = html.find("span", class_="prd_like_cnt").text
                    dic["ì´ë¯¸ì§€url"] = html.find("img", id="bigimg")['src']
                    dic["ë‚¨ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["ì—¬ì„±êµ¬ë§¤ë¹„ìœ¨"] = 0
                    dic["~18ì„¸"] = 0
                    dic["19ì„¸~23ì„¸"] = 0
                    dic["24ì„¸~28ì„¸"] = 0
                    dic["29ì„¸~33ì„¸"] = 0
                    dic["34ì„¸~39ì„¸"] = 0
                    dic["40ì„¸~"] = 0
                    li2.append(dic)
            except AttributeError:
                print("4ë²ˆ error")
                continue
            print("4ë²ˆ í”„ë¡œì„¸ìŠ¤")

    driver.close()
    return li2


# ë©€í‹°í”„ë¡œì„¸ì‹±
def multi_pro(f_url):
    code_list = [1, 2, 3, 4]
    pool = multiprocessing.Pool(4)
    func = partial(details_crawling, f_url)

    print('---- start _multiprocessing ----')
    multi1 = pool.map(func, code_list)
    pool.close()
    pool.join()
    print('---- end _multiprocessing ----')

    return multi1


# ë°ì´í„°í”„ë ˆì„í™”
def to_df(new_dic, multi_data):

    df1 = pd.DataFrame(new_dic)
    df1 = df1.replace(0, np.NaN)

    df2 = pd.DataFrame()
    for i in multi_data:
        temp_df = pd.DataFrame(i)
        df2 = pd.concat([df2, temp_df])
    df2 = df2.replace(0, np.NaN)

    df1 = df1.reset_index()
    df2 = df2.reset_index()
    last_df = pd.concat([df1, df2], axis=1)
    last_df = last_df.drop(['index'], axis=1)
    last_df = last_df.replace(0, np.NaN)

    return last_df


# ì„¸ë¶€ì •ë³´ ì´ë²¤íŠ¸ ì²˜ë¦¬
def crawling_option(details_box, new_dic, f_url):
    if details_box:
        multi_data = multi_pro(f_url)
        last_df = to_df(new_dic, multi_data)

        return last_df

    else:
        print(new_dic)
        df1 = pd.DataFrame(new_dic)
        print(df1)
        df1 = df1.replace(0, np.NaN)

        return df1


# ìµœì¢… í¬ë¡¤ë§ ì½”ë“œ (main)
def crwaling_fin(d_li):
    start = time.time()
    print("í¬ë¡¤ë§ ì‹œì‘í•©ë‹ˆë‹¤.")

    global li00
    global li11
    li00 = []
    li11 = []

    path = musinsa_url(d_li[0], d_li[1])
    url_box = 1
    name_box = d_li[2][0]
    price_box = d_li[2][1]
    sale_price_box = d_li[2][2]
    brand_box = d_li[2][3]
    details_box = d_li[2][4]
    # url_box = 1
    # name_box = 1
    # price_box = 1
    # sale_price_box = 1
    # brand_box = 1
    # details_box = 0

    new_dic, f_url = crawling(path, url_box, name_box, price_box, sale_price_box, brand_box)
    final_df = crawling_option(details_box, new_dic, f_url)

    print("time :", time.time() - start)
    print("í¬ë¡¤ë§ ì¢…ë£Œì…ë‹ˆë‹¤.")

    return final_df


# ------------------------------------------------------------------------------------------------
# ì´í›„ ìŠ¤íŠ¸ë¦¼ë¦¿ ì½”ë“œ ì‘ì„±
# í°íŠ¸ ì¡°ì •
d_li = []
matplotlib.rcParams['font.family'] = 'Malgun Gothic'
matplotlib.rcParams['axes.unicode_minus'] = False
st.set_page_config(layout="wide")


# íƒ€ì´í‹€
st.sidebar.image("https://d34u8crftukxnk.cloudfront.net/slackpress/prod/sites/6/Musinsa_B.png", width=250)


# ìƒí’ˆ ê²€ìƒ‰ì°½
st.sidebar.header("ìƒí’ˆê²€ìƒ‰")
key_word = st.sidebar.text_input('ìƒí’ˆëª…')
d_li.append(key_word)
page_num = st.sidebar.number_input('ê²€ìƒ‰ í˜ì´ì§€ ìˆ˜', min_value=1, max_value=50, value=1)
d_li.append(page_num)


# í¬ë¡¤ë§ ë°ì´í„° ë¶„ë¥˜
key_word2 = st.sidebar.write('ì–´ë–¤ ë°ì´í„°ë¥¼ í¬ë¡¤ë§ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?')
checked1 = st.sidebar.checkbox('ì œí’ˆëª…')
checked2 = st.sidebar.checkbox('ê°€ê²©')
checked3 = st.sidebar.checkbox('í• ì¸ê°€ê²©')
checked4 = st.sidebar.checkbox('ë¸Œëœë“œ')
checked5 = st.sidebar.checkbox('ì„¸ë¶€ì •ë³´')


# ë²„íŠ¼ì„¸ì…˜ ì²˜ë¦¬
if 'button_1' not in st.session_state:
    st.session_state.button_1 = False
if 'button_2' not in st.session_state:
    st.session_state.button_2 = False


def cb1():
    st.session_state.button_1 = True


def cb2():
    st.session_state.button_2 = True


# ì´ë²¤íŠ¸ ì²˜ë¦¬
d_li.append([int(checked1), int(checked2), int(checked3), int(checked4), int(checked5)])
print(d_li)
st.sidebar.button("í¬ë¡¤ë§", on_click=cb1)


# ì„ì‹œ ë°ì´í„°
# df = pd.read_csv("C:/Users/playdata/PycharmProjects/pythonProject/123/crawling_musinsa_detail_conc.csv")


# í¬ë¡¤ë§ ë²„íŠ¼ ì²˜ë¦¬
if st.session_state.button_1:
    with st.spinner('Wait for it...'):
        df = pd.read_csv("C:/Users/playdata/PycharmProjects/pythonProject/123/crawling_musinsa_detail_conc.csv")
        st.balloons()
        st.success('Done!')
        st.subheader("DataFrame ë°ì´í„°")
        st.dataframe(df, use_container_width=False)


# ë°ì´í„° ì €ì¥ ë°©ì‹ ì„ íƒ
radio2_options = ['xlsx', 'csv', 'txt', 'json']
radio2_selected = st.sidebar.radio('ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?', radio2_options, index=1)
st.sidebar.write('í˜„ì¬ ì„ íƒ:', radio2_selected)
st.sidebar.button("ì €ì¥í•˜ê¸°", on_click=cb2)

try:
    if st.session_state.button_2 and radio2_selected == radio2_options[0]:
        df.to_excel("musinsa_df.xlsx", index=False)

    elif st.session_state.button_2 and radio2_selected == radio2_options[1]:
        df.to_csv("musinsa_df.csv", encoding="ANSI", index=False)

    elif st.session_state.button_2 and radio2_selected == radio2_options[2]:
        df.to_csv("musinsa_df.txt", sep='\t', encoding="utf-8", index=False)

    elif st.session_state.button_2 and radio2_selected == radio2_options[3]:
        df.to_json("musinsa_df_table.json", force_ascii=False, index=False, orient='table')
        df.to_json("musinsa_df_columns.json", force_ascii=False, orient='columns')
        df.to_json("musinsa_df_index.json", force_ascii=False, orient='index')
        df.to_json("musinsa_df_split.json", force_ascii=False, index=False, orient='split')

except NameError:
    st.error('í¬ë¡¤ë§ì„ ë¨¼ì € ì§„í–‰í•´ì£¼ì„¸ìš”!!', icon="ğŸš¨")


[col1, col2] = st.columns(2)

with col1:
    st.subheader("EAOOOOOOOOOOOOOOOOOOO")
    image_url = "https://i1.sndcdn.com/artworks-000705954469-k42z0p-t500x500.jpg"
    st.image(image_url, width=450)

with col2:
    st.subheader("AWOOOOOOOOOOOOOOOOOOO")
    image_url = "https://preview.redd.it/b4ww0dmctns01.jpg?auto=webp&s=7135e279cffd58eca2650285e451a0013dd91b2b"
    st.image(image_url, width=450)
